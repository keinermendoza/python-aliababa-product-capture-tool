# Alibaba Scraper

Sistema que automatiza o processo de coleta de dados de produtos e fornecedores na plataforma Alibaba, com foco na busca e organiza√ß√£o de cota√ß√µes de fornecedores internacionais.

A solu√ß√£o combina uma **extens√£o personalizada do Google Chrome**, um **backend em Python com Flask**, um **banco de dados relacional** e uma **interface web** para visualiza√ß√£o e exporta√ß√£o dos dados coletados.

---

## Instala√ß√£o e Uso da Extens√£o do Google Chrome

A extens√£o utilizada neste projeto √© uma **extens√£o personalizada do Google Chrome**, carregada em modo desenvolvedor.  
Ela √© respons√°vel por coletar automaticamente os dados de produtos e fornecedores diretamente do site do Alibaba.

‚ö†Ô∏è **Importante:**  
Antes de utilizar a extens√£o, √© **obrigat√≥rio** criar uma **Request for Quotation** na aplica√ß√£o web local desenvolvida com Flask.  
Todas as *quotations* coletadas pela extens√£o **devem estar associadas a uma request for quotation ativa**.

---




### 1. Iniciar o servidor Flask

Certifique-se de que o servidor esteja em execu√ß√£o:

```bash
cd python-aliababa-product-capture-tool
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
flask start_db
flask run
```

A aplica√ß√£o web ficar√° dispon√≠vel localmente (por padr√£o em `http://localhost:5000`).

---

### 2. Criar uma Request for Quotation

1. Acesse a aplica√ß√£o web local no navegador.
2. Na tela principal, crie uma nova **Request for Quotation**, informando:
   - Um t√≠tulo
   - A quantidade desejada
3. Ap√≥s criada, a request ser√° automaticamente marcada como **ativa**.

üìå Apenas **uma request for quotation ativa** pode receber novas quotations.  
A extens√£o sempre associar√° os dados coletados √† request ativa no momento do scraping.

---

### 3. Instalar a extens√£o do Google Chrome

1. Abra o Google Chrome.
2. Acesse:
   ```
   chrome://extensions
   ```
3. Ative o **Modo do desenvolvedor** no canto superior direito.
4. Clique em **‚ÄúCarregar sem compacta√ß√£o‚Äù** (*Load unpacked*).
5. Selecione a pasta **`product-scraper`** localizada na raiz do projeto.

Ap√≥s esses passos, a extens√£o estar√° instalada e pronta para uso.

---

### 4. Utilizar a extens√£o para coletar dados

1. Certifique-se de que:
   - O servidor Flask esteja em execu√ß√£o.
   - Exista uma **request for quotation ativa** na aplica√ß√£o web local.

2. Acesse o site do **Alibaba** e navegue at√© a **p√°gina de um produto espec√≠fico**  
   (n√£o utilize p√°ginas de listagem ou resultados de busca).

3. Com a p√°gina do produto aberta, utilize a **extens√£o do Google Chrome**, conforme demonstrado no v√≠deo explicativo do projeto.

https://github.com/user-attachments/assets/ee6fb04f-e7f6-42ea-a42d-6c351ec54549


## Vis√£o Geral da Solu√ß√£o

O sistema √© composto por v√°rias partes que trabalham de forma integrada:

- Extens√£o do Google Chrome para coleta autom√°tica de dados diretamente das p√°ginas de produtos do Alibaba.
- Servidor web local em Python (Flask) respons√°vel por receber, processar e persistir os dados.
- Banco de dados relacional (SQLite) para armazenar requests for quotation e quotations.
- Comunica√ß√£o em tempo real com Socket.IO para sincroniza√ß√£o autom√°tica da interface.
- Exporta√ß√£o de dados para CSV para gera√ß√£o de relat√≥rios.

---

## Estrutura do Projeto

```
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ csv
‚îú‚îÄ‚îÄ migrations
‚îÇ   ‚îî‚îÄ‚îÄ 0001_alter_table_names.sql
‚îú‚îÄ‚îÄ product-scraper
‚îÇ   ‚îú‚îÄ‚îÄ content.js
‚îÇ   ‚îú‚îÄ‚îÄ manifest.json
‚îÇ   ‚îú‚îÄ‚îÄ popup.html
‚îÇ   ‚îî‚îÄ‚îÄ popup.js
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ repository.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ schema.py
‚îú‚îÄ‚îÄ sheets.py
‚îú‚îÄ‚îÄ templates
‚îÇ   ‚îú‚îÄ‚îÄ quotation_edit.html
‚îÇ   ‚îú‚îÄ‚îÄ quotation_list.html
‚îÇ   ‚îî‚îÄ‚îÄ request_for_quotation_list.html
‚îî‚îÄ‚îÄ utils.py
```

---

## product-scraper

Diret√≥rio que cont√©m os arquivos da **extens√£o personalizada do Google Chrome** respons√°vel pelo scraping.

O arquivo mais importante √© o **`content.js`**, onde:
- Os dados do produto e do fornecedor s√£o extra√≠dos diretamente do DOM da p√°gina do Alibaba.
- As informa√ß√µes coletadas s√£o enviadas para o servidor Flask local atrav√©s de uma requisi√ß√£o HTTP (`/webhook`).

---

## app.py

Arquivo principal da aplica√ß√£o backend.

Responsabilidades principais:

- Inicializa o servidor Flask.
- Configura CORS para permitir requisi√ß√µes apenas do dom√≠nio do Alibaba.
- Define o endpoint `/webhook`, respons√°vel por:
  - Receber os dados enviados pela extens√£o.
  - Persistir as cota√ß√µes no banco de dados.
  - Emitir eventos via Socket.IO para atualizar a interface em tempo real.
- Define endpoints para:
  - Criar e listar **requests for quotation**.
  - Listar **quotations** associadas a uma request.
  - Gerar arquivos CSV com as cota√ß√µes.
- Define um comando customizado do Flask para inicializar o banco de dados.

---

## templates

Cont√©m os templates HTML que comp√µem o frontend da aplica√ß√£o.

- **request_for_quotation_list.html**  
  Permite criar e ativar uma *request for quotation*.  
  Lista todas as requests cadastradas, exibindo a quantidade de quotations associadas a cada uma.

- **quotation_list.html**  
  Exibe todas as *quotations* relacionadas a uma *request for quotation* espec√≠fica.  
  Esses dados s√£o extra√≠dos diretamente do site do Alibaba.

- **quotation_edit.html**  
  Exibe todas as propriedades de uma *quotation*, permitindo a edi√ß√£o de algumas delas.

---

## schema.py

Define o esquema do banco de dados utilizando **SQLAlchemy Core**.

Este m√≥dulo atua como a camada de defini√ß√£o das tabelas e da estrutura do banco, fazendo a ponte entre o banco de dados relacional e a aplica√ß√£o Python.

---

## repository.py

Cont√©m a classe **`SQLAlchemyRepository`**, respons√°vel por **todas as intera√ß√µes com o banco de dados**.

- Toda comunica√ß√£o com o banco deve ser feita exclusivamente atrav√©s desta classe.
- Centraliza a l√≥gica de persist√™ncia, consultas e atualiza√ß√µes.

---

## sheets.py

Respons√°vel pela exporta√ß√£o dos dados.

Atualmente:
- Exporta as cota√ß√µes para o formato **CSV**.
- Os arquivos gerados s√£o salvos no diret√≥rio `csv/`.

---

## utils.py

M√≥dulo de utilidades auxiliares.

- **slugify**  
  Gera strings seguras para nomes de arquivos e URLs.

- **copy_buyer_script_to_clipboard**  
  Copia automaticamente um script com informa√ß√µes do comprador para a √°rea de transfer√™ncia.

---

## Requisitos

- Python 3.10+
- Google Chrome
- Git (opcional)

---

## Vari√°veis de Ambiente

```
BUYER_NAME
BUYER_ADDRESS
```

---

## Observa√ß√µes Finais

Projeto desenvolvido a partir da combina√ß√£o entre estudo acad√™mico e aprendizado autodidata.
